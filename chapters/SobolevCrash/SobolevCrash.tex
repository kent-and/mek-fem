\chapter{Crash course in Sobolev Spaces}

\label{chap-sobolev}
\section{Introduction}

Sobolev spaces are fundamental in the analysis of partial differential equations and also for 
finite element methods. Many books provide a detailed and comprehensive analysis of these spaces
that in themselves deserve significant attention if one wishes to understand the foundation that
the analysis of partial differential equations relies on. In this chapter we will however not
provide a comprehensive mathematical description of these spaces, but rather try to provide 
insight into their use. 

We will here provide the definition of these spaces. Further we will show typical functions, useful
for finite element methods,  that are in some but not all spaces. We also show how different norms
capture different characteristics.   

\section{Norms, inner products and Sobolev spaces}

Function spaces are vector spaces where the vectors are functions. Obviously, this makes
sense as functions can be added to each other, multiplied with scalars etc.,  so all 
requirements for a vector space can be easily verified. 

We will use $L^p$ spaces which are  defined as follows.  
Let $u$ be a scalar valued function on the domain $\Omega$, which for 
the moment will be assumed to be the unit interval $(0,1)$. Then the $L^p$ norm on $\Omega$ is:    
\[
\|u\|_p = (\int_0^1 |u|^p \, dx)^{1/p} .    
\]
$L^p(\Omega)$ consists of all functions for which $\|u\|_p < \infty$. 
Sobolev spaces generalize $L^p$ spaces by also including the derivatives.  
On the unit interval, the $W^{k,p}$ norm is defined as  
\begin{equation}
\label{Wpk}
\|u\|_{k,p} = (\int_\Omega \sum_{i \le k} |(\frac{\partial^i u}{\partial x^i}) |^p \, dx)^{1/p} .    
\end{equation}
Then the Sobolev space $W^{k,p}(\Omega)$ consists of 
all functions with $\|u\|_{k,p} < \infty$. $W^{k,p}$ is a so-called Banach space - that is 
a complete\footnote{We will not go into the details of complete spaces in this book, but remark that
the space of real numbers is complete, while the space of rational numbers is not. } normed vector space. 
Notice that the derivative is defined \emph{weakly}, ie. in the sense that existence is required only in $L^p$. For instance, 
a function may have derivatives that are infinite on zero measures, such as point-wise infinities. 

The corresponding semi-norm, which only include the highest order derivative is 
\begin{equation}
\label{semiWpk}
|u|_{k,p} = (\int_\Omega  |(\frac{\partial^k u}{\partial x^k})   u|^p \, dx)^{1/p} .    
\end{equation}
The case $p=2$ is special in the sense that not only a norm is defined but also an inner product.    
The Banach space then forms a Hilbert space and these named with $H$ in Hilbert's honor. 
That is $H^k(\Omega) = W^{k,2}(\Omega)$. The inner product between the functions $u$ and $v$ is:  
\[
(u, v)_{k} = \sum_{i \le k} \int_\Omega \frac{\partial^i u}{\partial x^i} \frac{\partial^i v}{\partial x^i} \,  dx.    
\]

For the most part, we will employ the two spaces $L^2(\Omega)$ and $H^1(\Omega)$, but also other
spaces will be used from time to time.  
The difference between the norm in $L^2(\Omega)$ and $H^1(\Omega)$ is illustrated in the following example.   


\kent{Need an example env easier to reference to.}

\begin{exmp}{\textbf{Norms of $sin(k \pi x)$.}} \label{ex:sin:norm} 
Consider the functions $u_k = \sin(k \pi x)$ on the unit interval. 
The norms of $u_k$ are shown in Table \ref{ex:sin:tab:norms}, while 
Fig. \ref{fig:sin} shows the function for $k=1$ and $k=10$. Clearly, the $L^2$ and $L^7$ behave similarly in the sense
that they remain the same as $k$ increases. On the other hand, the $H^1$ norm of $u_k$
increases dramatically as $k$ increases. The following code shows how the 
norms are computed using FEniCS.         

\begin{python}
from dolfin import *

N = 10000 
mesh = UnitInterval(N)
V = FunctionSpace(mesh, "Lagrange", 1)

for k in [1, 100]: 
  u_ex = Expression("sin(k*pi*x[0])", k=k)
  u = project(u_ex, V)

  L2_norm = sqrt(assemble(u**2*dx))
  print "L2 norm of sin(%d pi x) %e " % (k, L2_norm) 

  L7_norm = pow(assemble(abs(u)**7*dx), 1.0/7)
  print "L7 norm of sin(%d pi x) %e " % (k, L7_norm) 

  H1_norm = sqrt(assemble(u*u*dx+inner(grad(u), grad(u))*dx))
  print "H1 norm of sin(%d pi x) %e" % (k, H1_norm) 
\end{python}

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}  \hline
$k \backslash norm $ & $L^2$ &   $L^7$ &  $H^1$ \\ \hline
1 & 0.71 &    0.84   & 2.3      \\ \hline
10  & 0.71 &   0.84    & 22    \\ \hline
100  & 0.71 &   0.84  &  222   \\ \hline
\end{tabular}
\caption{ The $L^2$, $L^7$, and $H^1$ norms of $sin(k \pi x)$ for $k=1, 10, 100$.   }
\label{ex:sin:tab:norms}
\end{center}
\end{table}


\end{exmp}



\begin{figure}
\label{fig:sin}
\begin{center}
\includegraphics[width=5cm]{chapters/SobolevCrash/sin.png}
\includegraphics[width=5cm]{chapters/SobolevCrash/sin10.png}
\caption{Left picture shows $\sin(\pi x)$ on the unit interval, while 
the right picture shows $\sin(10 \pi x)$. }\label{fig:sin}
\end{center}
\end{figure}



\section{Poincare's lemma}
\label{sec:poincare}
Poincare's inequality states that for $u\in  H^1_{0, D}(\Omega)$ we have
\begin{equation} 
\label{poincare}
\|u\|_{L^2(\Omega)}  \le C \| \nabla u\|_{L^2(\Omega)} 
\end{equation} 
We will not prove this inequality in this book (see e.g. ~\cite{evans2022partial}),  
but we note that it can be motivated by considering the Taylor series approximation. 
From basic calculus we know that: 
\[
|f(x+h)|  \le |f(x)| + h \max_{x \le x^* \le x+h } \,  |f'(x^*) | 
\]
Chosing $x$ on $\partial \Omega_D$ makes $f(x)=0$ and hence $f(x+h)$ can be bounded by the derivative
of $f$. Of course, Poincare's result provides a bounds on the integral rather than the pointwise sense 
of Taylor, but the results are similar. 


\section{Spaces and sub-spaces, norms and semi-norms}

The Sobolev space with $k$ derivatives in $L_2(\Omega)$ was denoted by $H^k(\Omega)$. The subspace
of $H^k$ with $k-1$ derivatives equal to zero at the boundary is denoted 
$H^k_0(\Omega)$. For example, $H^1_0(\Omega)$ consists of all functions in $H^1$ that are zero 
at the boundary. Similarly, we may also defined a subspace  
$H^1_g(\Omega)$ which consists of all functions in $H^1(\Omega)$ that are equal to the function $g$ 
on the boundary. 

The norm $\|\cdot\|_{p,k}$ defined in \eqref{Wpk} is a norm
which means that $\|u\|_{p,k} > 0$ for all $u\not=0$.    
On the other hand  $|\cdot|_{p,k}$ is a semi-norm, meaning 
that $|u|_{p,k} \ge  0$ for all $u$.   
The space $H^1(\Omega)$ is defined by the norm 
\[
\|u\|_1 = \|u\|_{H^1}  = (\int_\Omega u^2 + (\nabla u)^2 \, dx)^{1/2}  
\]
and contains all functions for which $\|u\|_1 \le \infty$. 
The space has an associated inner product. Let $u, v \in H^1(\Omega)$ then the inner product is 
\[
(u, v)_1 = (u, v)_{H^1}  = \int_\Omega u v  + \nabla u \nabla v  \, dx  .  
\]
 
The corresponding semi-norm $|\cdot|_1$ defined as 
\[
|u|_1 = \|u\|_{H_0^1} =   (\int_\Omega  (\nabla u)^2 \, dx)^{1/2}  
\]
We will see later that Poincare's lemma
ensures that $\|\cdot\|_1$ and  $|\cdot|_1$ are equivalent norms on  the subspace $H^1_0$ (see Exercise \ref{ex:poincare}).  
On $H^1_0$ we have the associated inner product  
\[
(u,v)_{H^1_0 } = \int_\Omega \nabla u \nabla v  \, dx  .  
\]




\section{Examples of Functions in Different Spaces}

The above functions $\sin(k \pi x)$ are smooth functions
that for any $k$ are infinitely many times differentiable. 
They are therefore members of any Sobolev space as long as $\Omega$ is bounded.  

On the other had, the step function in upper picture in Figure \ref{fig:piecewise} is 
discontinuous in $x=0.2$ and $x=0.4$. Obviously, the function is in $L^2(0,1)$, but 
the function is not in $H^1(0,1)$ since the derivative of the function  consists of Dirac's 
delta functions $\delta_{0.2}$  and $-\delta_{0.4}$ 
that are $\infty$ at $x=0.2$ and $-\infty$ in $x=0.4$\footnote{We remark that 
for $y \in \Omega$ 
$\int_\Omega \delta_y \, dx = 1 $ However, $\int_\Omega \delta^2_y \, dx = \infty $.  
}
. 

The hat function in the lower picture in Figure \ref{fig:piecewise} is a typical
first order finite element function. The function is in both $L^2(0,1)$ and $H^1(0,1)$ (see
Exercise \ref{ex:piecewise}).         
In general, functions in $H^q(0,1)$ are required to be in $C^{q-1}(0,1)$, where $C^k(0,1)$ is the
class  where the $k$'th derivatives exist and
are continuous on the unit interval.   

\begin{remark}
In $C^k(\Omega)$ a function and its $k$ derivatives can be evaluated at any given point in $\Omega$ and  
the result will be a finite value. How about functions in $H^k(\Omega)$? In general only functions
in $H^k(\Omega)$ for $k\ge 2$ allow pointwise evaluation. The exact relationship between 
continous functions and Sobolev spaces are covered by the  Sobolev Embedding Theorem~\cite{evans2022partial}, which will not 
be covered in this book. 
Anyways, in $L^2(\Omega)$ or $H^1(\Omega)$ we should make note and be careful when we perform pointwise evaluations. 
\end{remark}


\begin{figure}
\begin{center}
%\includegraphics[width=6cm]{chapters/SobolevCrash/sin.png}
\includegraphics[width=6cm]{chapters/SobolevCrash/pc.png}
\includegraphics[width=6cm]{chapters/SobolevCrash/pl.png}
\caption{The upper picture shows a piecewise function, discontinuous at $x=0.2$ and $x=0.2$, while
the lower picture shows a linear function that is continuous. }  
\label{fig:piecewise}
\end{center}
\end{figure}

\section{Sobolev Spaces and Polynomial Approximation}
From Taylor series approximation we know that  $f(x+h)$ may be approximated
by $f(x)$ and a polynomial in $h$ that depends on the derivatives of $f$. 
To be precise, 
\[
|f(x+h) - P_{h,k} f (x) |  \le  \mathcal{O}(h^{k+1}) .    
\]
Here, $P_{h,k} f(x)$ is the following polynomial of degree $k$ in $h$, 
\[
P_{h,k} f(x) = f(x) + \sum_{n=1}^k \frac{f^{(n)}(x)}{n!} h^n . 
\]
where $f^{(n)}$ denotes the $n$'th derivative of $f$. 

In general, approximation by Taylor series bears strong requirement on the smoothness 
of the solution which needs to be differentiable in a point-wise sense.   
Hence, we need to defined the polynomials in an alternative way, but the
results are similar.  We conclude, without proof, that  in Sobolev spaces we have the very useful approximation property  
\begin{equation}
\label{bramblehilbert}
|u - P_m u|_{k,p} \le C h^{m-k} |u|_{m,p} \quad \mbox{ for } k=0,\ldots,m \mbox{ and }  p\ge 1. 
\end{equation}
This property is used extensively in analysis of finite element methods and
is called the Bramble-Hilbert lemma for $k\ge 2$. The case $k=1$ was included by a special interpolation operator by 
Clement, the so-called Clement interpolant. 
For proof, see e.g. \cite{braess2007finite, brenner2008mathematical}.  
In Chapter \ref{element}, Example \ref{lagrange:element} we will consider an explicit construction (the Lagrange interpolation) in more detail. 



\section{Eigenvalues and Finite Element Methods}
\label{sec:eig and fem}

Eigenvalues and eigenvectors are enormously useful for understanding the properties 
of matrices and operators. Let us therefore consider how to compute these when using the finite element method. 

It is well known that for $-\Delta$  with homogenous Dirichlet conditions on the unit
interval $(0,1)$, 
the eigenvalues 
and eigenvectors
are $(\pi k)^2$ and $sin(\pi k x)$, $k=1, \ldots, \infty$, respectively. 
It is natural to expect that the eigenvalues in the discrete setting
approximate the continuous eigenvalues such that 
the minimal eigenvalue is $\approx \pi^2$, while the maximal eigenvalue
is $\approx \pi^2 /h^2$, where $k=1/h$ corresponds to the highest frequency that may be represented
on a mesh with element size $h$.  
Computing the eigenvalues of the finite element stiffness matrix in FEniCS as\footnote{
We use the \emp{assemble\_system} function to enforce the Dirichlet
condition in symmetric fashion.}, 
\begin{python}
A = assemble_system(inner(grad(u), grad(v))*dx, Constant(0)*v*dx, bc)
\end{python}
reveals that the eigenvalues are differently scaled. In fact, the minimal 
eigenvalue is $\approx \pi^2 h$ and that the maximal eigenvalue is  $\approx \pi^2/h$, see Fig.\ref{fig:geneig} (red and blue curves).      
The reason is that the finite element method introduces a mesh-dependent 
scaling due to the fact that it is a variational method. 
Specifically, if we want to approximate a $f$ as $\sum_j f_j N_j$ we notice
that we should calculate $f_j$ in a variational sense as follows: 
\[
 \sum_j \int_\Omega f_j N_j N_i \, dx = \int_\Omega f N_i \, dx  .  
\]
Hence, we notice here that, we are solving a linear system
\[
	M f = b, \mbox{ with } M_{ij} = \int_\Omega N_j N_i \, dx \mbox{ and } b_i = \int_\Omega f N_i \, dx  
\]
Both $\{f_i\}_i$ and $\{b_i\}_i$ are \emph{representations} of $f$, often called the \emph{nodal} and \emph{dual} representations~\cite{mardal2011preconditioning}. They scale differently since
the entries of the mass matrix scale with the size of the elements in the mesh. 

To estimate the continuous eigenvalues, we  need to make sure that both the left- and right-hand sides are in the same representation, either nodal
or dual. Hence, for a nodal representation of the eigenvalues and eigenvectors, we should consider the generalized eigenvalue problem, 
\begin{equation} 
\label{geneig}
A x = \lambda M x,   
\end{equation} 
where $A$ is the above mentioned stiffness matrix and $M$ is the mass
matrix (or the finite element identity matrix) 
\begin{python}
M = assemble_system(inner(u*v*dx, Constant(0)*v*dx, bc)
\end{python}

Figure \ref{geneig} shows the eigenvalues of $-\Delta$, $A$, and  
\eqref{geneig} based on the following code: 
\begin{python}
from dolfin import *
import numpy
from scipy import linalg, matrix

def boundary(x, on_boundary): return on_boundary

for N in [100, 1000]: 
  mesh = UnitIntervalMesh(N)
  V = FunctionSpace(mesh, "Lagrange", 1)
  u = TrialFunction(V)
  v = TestFunction(V)

  bc = DirichletBC(V, Constant(0), boundary)
  A, _ = assemble_system(inner(grad(u), grad(v))*dx, Constant(0)*v*dx, bc)
  M, _ = assemble_system(u*v*dx, Constant(0)*v*dx, bc)

  AA = matrix(A.array())
  MM = matrix(M.array())

  k = numpy.arange(1, N, 1)
  eig = pi**2*k**2 

  l1, v  = linalg.eigh(AA)
  l2, v  = linalg.eigh(AA, MM)

  print "l1 min, max ", min(l1), max(l1) 
  print "l2 min, max ", min(l2), max(l2) 
  print "eig min, max ", min(eig), max(eig) 

  import pylab 
  pylab.loglog(l1[2:], linewidth=5) # exclude Dirichlet values 
  pylab.loglog(l2[2:], linewidth=5) # exclude again 
  pylab.loglog(eig, linewidth=5)
  pylab.legend(["eig(A)", "eig(A,M)", "cont. eig"], loc="upper left")
  pylab.show()
\end{python}

\begin{figure}
\begin{center}
\includegraphics[width=6cm]{chapters/SobolevCrash/eig.png}
\caption{A log-log plot of the eigenvalues of $A$, $M^{-1} A$, and $-\Delta$. }  
\label{fig:geneig}
\end{center}
\end{figure}
From Figure \ref{fig:geneig} we see that that the eigenvalues
of \eqref{geneig} and $-\Delta$ are close, while the eigenvalues
of $A$ is differently scaled.  
We remark that we excluded the two smallest eigenvalues in the discretized problems 
as they correspond to the Dirichlet conditions. 
%We remark 
%that we use \emp{pytave}, the Python interface of Octave because
%\emp{numpy} is not trustworthy for eigenvalue computations on 
%"large" linear systems. A description of \emp{pytave} is found in~\cite{mardal2013combining}.  

\section{Linear functionals and dual spaces}

A functional on a vector space $V$ is an object $L(\cdot)$ that when given a
vector $v\in V$, returns a real number. 
For example let $V=\mathbb{R}^n$ and the inner-product in use be 
the $l_2$ inner product, ie.  $(x, y)_{l_2} = \sum_i x_i y_i$. 
Then any vector $z\in \mathbb{R}^n$ together
with $l_2$ inner product forms a linear functional on $V$ as
\[
L_z(x)  = (z, x)_{l_2} .  
\]
It is easy to verify that $L_z$ is linear and bounded. 

By choosing $z$ appropriately we may define linear functionals
that for example 1) returns the $i$'th component of the vector, i.e. by letting $z=e_i$ where
$e_i$ is the $i$'th unit vector or 2) computes the sum of a vector, i.e. by letting $z=(1,\ldots, 1)$. 
The mentioned functionals 1) and 2) are linear, but examples of non-linear functionals may e.g. be
sums of powers of the components, ie.  
\[
L_p (v) = \sum_i v_i^p. 
\]

The dual space of $V$, written as $V^*$,  is the space of all linear functionals on $V$. 
To define we need the definition of the dual norm. 

\begin{defin}{\textbf{The dual norm and the corresponding dual space.}} \\
\label{dualnorm}
\noindent
Let $(\cdot, \cdot)_V$ be an inner product over the Hilbert
space $V$ and let $f(\cdot)$ be a linear functional on $V$.   
The dual norm of $f$ is 
\[
	\|f\|_{V^*} = \sup_{v\in V} \frac{f(v)}{\sqrt{(v,v)_V}} .    
\]
The space $V^*$ consists of all $f$ such that $\|f\|_{V^*} < \infty$.  
\end{defin}

We remark that the dual space of $\mathbb{R}^n$ is $\mathbb{R}^n$, ie. $(\mathbb{R}^n)^* = \mathbb{R}^n$. 
In general however, the dual space $V^*$ may in general be different from $V$.   

Functionals on function spaces are analogous to functionals on $\mathbb{R}^n$. For example, the
example functionals 1) and 2) above directly maps to corresponding functionals on function spaces
with the $l_2$ inner product replaced by the $L^2(\Omega)$ inner product. 
That is, for example the counter-part of example 1) above, the $i$'th component of a vector, is function evaluation in a point $x_i$.  
The corresponding functional is: 
\[
L_1 (u)  = \int_\Omega u (x) \delta_{x_i} (x) \, dx   , 
\]
with $\delta_{x_i}$ being the Dirac delta function associated with $x_i$.  Similarly, the example 2) 
the summation of all entries in a vector simply corresponds to the integral of the function 
\[
L_2 (u)  = \int_\Omega u (x)  \, dx   .  
\]


We notice immediately that the situation is more complicated for function spaces than for $\mathbb{R}^n$. 
There are obviously functions in which pointwise evaluation 
and/or integrals are not well defined. Consider for example the
function $f(x) = x^\alpha$. Clearly, if $\alpha < 0$ then $f(0) = \infty$. Furtermore, 
if $\alpha \ge 0$ and  our domain of interest is $\mathbb{R}^+$ then
\[
\int_0^\infty f(x) \, dx = \infty .  
\]
Hence, which linear functionals that belongs to which dual spaces are not always easy to grasp 
and may require careful considerations. 
Let us for example, 
consider the one-dimensional function space $V$ spanned by  $x^\alpha$ and associated with the $L^2$ inner product.   
Then clearly, $g(x) = x^\beta$ and the associated  
linear functional $L_g(\cdot)$ is in $V^*$ if $\alpha + \beta < -1$ since
\[
L_g(f) = \int_0^\infty f(x) g(x) \, dx   < \infty  
\]

The dual space of $H^1_0$ with respect to the $L^2$ inner product 
is commonly refered to as 
$H^{-1}$. Formally, by using the above defintion, its norm is  
\[
\|f\|_{-1} = \sup_{v\in H^1_0} \frac{f(v)}{|v|_1} .    
\]
Clearly, the  $H^{-1} \subset L^2$, using 
the inequalities
of 
Cauchy–Schwarz and  
Poincare, see Sec. \ref{sec:poincare}, since
\[
\|f\|_{-1} = \sup_{v\in H^1_0} \frac{f(v)}{|v|_1} = \sup_{v\in H^1_0} \frac{\int_\Omega fv \,  dx}{\sqrt{\int_\Omega (\nabla v)^2 \, dx }}  \le  \sup_{v\in H^1_0} \frac{\|f \|_{0} \|v\|_0}{|v|_1} \le C \|f \|_{0}     
\]
In fact, $H_0^1(\Omega) \subset L^2 (\Omega) \subset H^{-1}(\Omega)$.   

A major result in functional analysis, the \emph{Riesz representation theorem}, 
states that if $V$ is a Hilbert space with the inner product $(\cdot, \cdot)_V$ then 
for every $f\in V^*$ there is a corresponding $u \in V$ such that
\[
(u, v)_V = f(v), \quad \forall v \in V.   
\]
Furthermore, 
\[
\|u\|_V = \|f\|_{V^*} . 
\]


\section{Negative and Fractional Norms}

As will be discussed more thoroughly later, 
$-\Delta$ is a symmetric positive operator on $H^1_0$ and can be thought of 
as an infinite dimensional matrix that is symmetric and positive.  
Indeed, in the concrete case of $\Omega = (0,1)$, in Sec.~\ref{sec:eig and fem} we discussed the spectral 
decomposition of $-\Delta$ in the continuous case along with the corresponding discretization using finite elements. 
Let us exploit the spectral decomposition of $-\Delta$ to make sense of negative, fractional and dual norms in 
the simple example where we have special decomposition. 

It is also know from Riesz representation theorem that if
$u$ solves the problem
\begin{eqnarray*}
-\Delta u &=& f, \quad \mbox{ in } \Omega, \\   
        u &=& 0, \quad \mbox{ on } \partial \Omega  
\end{eqnarray*}
then 
\begin{equation}
\label{riesz}
|u|_1 = \|f\|_{-1} . 
\end{equation}
This implicitly define the $H^{-1}$ norm, although
the definition then requires the solution of a Poisson problem.   
For example, in the previous example where
$u_k = sin(k\pi x)$
and the associated right hand side corresponding to the eigenfuction is  $f_k = -\Delta u_k = (k \pi)^2 sin(k\pi x) = (k \pi)^2 u_k $. 
We  have already estimated  
that $|u_k|_1 = \frac{\pi k}{\sqrt{2}}$ and therefore 
$\|u_k\|_{-1} =  \frac{1}{\sqrt{2} k \pi }$.  
We notice that the $\|\cdot\|_1$ norm weights high frequency oscillations heavily (linear in $k$), whereas
the $\|\cdot\|_{-1}$ does not (it scales as $1/k$). On the other hand $L^2$ weights oscillations at all frequencies
equally.   \label{sinkpnorms} 

Let us now generalize these considerations and  consider a matrix 
(or differential operator) $A$ which is
symmetric and positive. $A$ has positive and real
eigenvalues and defines an inner product which may
be represented in terms of eigenvalues and eigenfunctions. Let     
$\lambda_i$ and $u_i$ be the eigenvalues and eigenfunctions
such that 
\[
A u_i = \lambda_i u_i  
\]
Then, $x$ may be expanded in terms of the eigenfunctions $u_i$ as
$x=\sum_i c_i u_i$, where $c_i = (x, u_i)$,  and we obtain 
\[
(x,x)_A = (A x, x) = (A \sum_i c_i u_i  , \sum_j c_j u_j)     
= (\sum_i \lambda_i  c_i u_i  , \sum_j c_j u_j)  
\]
Because $A$ is symmetric, the egenfunctions $u_i$  are orthogonal to each other
and we may choose a normalized basis such that $(u_i, u_j) =  \delta_{ij}$.
With this normalization, we simply obtain
\[
\|x\|_A^2 = (x,x)_A = (A x, x) = (A \sum_i c_i u_i  , \sum_j c_j u_j)     
= \sum_i \lambda_i  c_i^2    
\]


A generalization of the $A-$inner product (with corresponding norm) to 
a $A^q-$inner product that allow for both negative and franctional $q$  
is then as follows
\begin{equation}
\label{qnorm}
\|x\|_{A,q}^2 =  (x,x)_{A,q} =  \sum_i \lambda^q_i c_i^2.     
\end{equation}
Clearly, this definition yields 
that $|u_k|_1 = \frac{\pi k}{\sqrt{2}}$ and 
$\|u_k\|_{-1} = \frac{1}{\sqrt{2} k \pi }$, as above.  

As mentioned in Section \ref{sec:eig and fem}, 
care has to be taken in finite element methods if the discrete eigenvalues
are to correspond with the continuous eigenvalues. We will therefore detail
the computation of negative and fractional norms in the following.   
Let $\lambda_i$ and $u_i$ be the eigenvalues and eigenvectors of the following
generalized eigenvalue problem  
\begin{equation}
\label{gen_eig}
A u_i = \lambda_i M u_i 
\end{equation}
and let $U$ be the matrix with the eigenvectors as columns. 
The eigenvalues are normalized in the sense 
that 
\[
U^T M U = I 
\]
where $I$ is the identity matrix. 
We obtain 
\[
U^T A U = \Lambda \quad \mbox{ or } \quad A = M U \Lambda (M U)^T  ,  
\]
where $\Lambda$ is a matrix with the eigenvalues $\lambda_i$ on the diagonal.  
Hence also in terms of the generalized eigenvalue problem \eqref{gen_eig}
we obtain the $A-$norm as  
\[
\|x\|^2_A = x^T MU \Lambda (MU)^T x   
\]
and  
we may define fractional and negative norms in the same manner
as \eqref{qnorm}, namely that 
\[
\|x\|^2_{A,M,q} = x^T M U \Lambda^q (MU)^T x .     
\]

Defining negative and fractional norms in terms of eigenvalues and eigenvectors 
is convenient for small scale problems, but eigenvalue problems are computationally
demanding. It may, however, be tractable on subdomains, such as surfaces or interfaces 
of larger problems. There are however more    
efficient ways of computing such norms and operators, but those are beyond the scope of this
book. 


\begin{example}{Computing the $H^1$, $L^2$, and $H^{-1}$ norms} \label{sc:ex2} 

Let us consider $\Omega = (0,1)$ and $u_k = sin(\pi k x)$.  
Table \ref{table:H1L2Hm1:norms} shows the $H^1$, $L^2$, and $H^{-1}$ norms as computed
with \eqref{qnorm} with $q=1, 0$, and $-1$, respectively.  Comparing 
	the norms computed with the below code and shown in Table \ref{ex:sin:tab:norms} with the norms calculated 
analytically on page \pageref{sinkpnorms},  we see that the above definition \eqref{qnorm}
reproduces the $H^1$ and $L^2$ norms with $q=1$ and $q=0$, respectively.     
We also remark that while the $H^1$ norm increases as
$k$ increases, the  $H^{-1}$ norm demonstrates a corresponding decrease.  
Below we show the code for computing these norms. 


\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}  \hline
$k \backslash norm $ & $H^1, \ q=1  $ &   $L^2, \ q=0$ &  $H^{-1}, \ q=-1$ \\ \hline
1 &  2.2 &    0.71  &  0.22      \\ \hline
10  & 22 &    0.71   & 0.022   \\ \hline
100  & 222 &   0.71  & 0.0022   \\ \hline
\end{tabular}
\caption{ The $L^2$, $L^7$, and $H^1$ norms of $sin(k \pi x)$ for k=1, 10, and 100.   }
\label{table:H1L2Hm1:norms}
\end{center}
\end{table}


\begin{python}
from dolfin import *
from numpy import matrix, diagflat, sqrt
from scipy import linalg, random 

def boundary(x, on_boundary): return on_boundary

mesh = UnitIntervalMesh(200)
V = FunctionSpace(mesh, "Lagrange", 1)
u = TrialFunction(V)
v = TestFunction(V)
bc = DirichletBC(V, Constant(0), boundary)

A, _ = assemble_system(inner(grad(u), grad(v))*dx, Constant(0)*v*dx, bc)
M, _ = assemble_system(u*v*dx, Constant(0)*v*dx, bc)
AA = matrix(A.array())
MM = matrix(M.array())

l, v = linalg.eigh(AA, MM)
v = matrix(v)
l = matrix(diagflat(l))

for k in [1, 10, 100]: 
  u_ex = Expression("sin(k*pi*x[0])", k=k)
  u = interpolate(u_ex, V)
  x = matrix(u.vector().array())

  H1_norm = pi*k*sqrt(2)/2  
  print "H1 norm of sin(%d pi x) %e (exact)          " % (k, H1_norm) 
  H1_norm = sqrt(assemble(inner(grad(u), grad(u))*dx)) 
  print "H1 norm of sin(%d pi x) %e (|grad(u)|^2)    " % (k, H1_norm) 
  H1_norm = sqrt(x*AA*x.T)    
  print "H1 norm of sin(%d pi x) %e (x A x' )        " % (k, H1_norm) 
  W = MM.dot(v)
  H1_norm = sqrt(x*W*l*W.T*x.T)   
  print "H1 norm of sin(%d pi x) %e (eig)            " % (k, H1_norm) 

  print "" 

  L2_norm = sqrt(2)/2 
  print "L2 norm of sin(%d pi x) %e (exact)          " % (k, L2_norm) 
  L2_norm = sqrt(assemble(u**2*dx)) 
  print "L2 norm of sin(%d pi x) %e   |u|^2          " % (k, L2_norm) 
  L2_norm = sqrt(x*MM*x.T) 
  print "L1 norm of sin(%d pi x) %e (x M x' )        " % (k, L2_norm) 
  W = MM.dot(v)
  L2_norm = sqrt(x*W*l**0*W.T*x.T)   
  print "L2 norm of sin(%d pi x) %e (eig)            " % (k, L2_norm) 

  print "" 

  Hm1_norm = sqrt(2)/2/k/pi  
  print "H^-1 norm of sin(%d pi x) %e (exact)        " % (k, Hm1_norm) 
  Hm1_norm = sqrt(x*W*l**-1*W.T*x.T)  
  print "H^-1 norm of sin(%d pi x) %e (eig)          " % (k, Hm1_norm) 
  Hm1_norm = sqrt(x*MM*linalg.inv(AA)*MM*x.T)    
  print "H^-1 norm of sin(%d pi x) %e (x inv(A) x') " % (k, Hm1_norm) 
\end{python}

\end{example}


\begin{remark}{\textbf{Norms for $|q| > 1$.} } \\
The norm \eqref{qnorm} is well defined for any $|q| > 1  $, but will not 
correspond to the corresponding Sobolev spaces.    
\end{remark}


\section{Boundary conditions, Traces and Fractional Sobolev Spaces }

In order to make sense of for instance boundary conditions we need to 
restrict from a domain $\Omega$ to its lower dimensional boundary $\partial \Omega$. 
Hence, for instance
if $\Omega$ is the unit square in 2D then $\partial \Omega$ consists of the 1D lines that 
connect the vertices at the boundary of the unit square. However, a 
1D line is inifinitely thin and has no measure in 2D. 
As such, in $L^p(\Omega)$ the values at $\partial \Omega$ can be anything and still have no 
measurable impact on the $L_p(\Omega)$ norm of the function since the measure is zero. 

However, functions in the Sobolev spaces $W^{k,p}$ have extra smoothness in terms of the requirements on derivatives. 
Therefore,
values at lower dimensional manifolds may sometimes be well-defined. In fact, we need to 
be able to assign values on lower dimensional manifolds to enforce Dirichlet conditions. 
In order to make sense of values at lower 
dimensional manifolds we need the mathematical constructs of traces and fractional 
Sobolev spaces. Specifically, let $T:\Omega \rightarrow \partial \Omega$ be the 
the so-called trace operator and $u \in w^{k,p}(\Omega)$ then from Sobolev space theory
it is well known that $Tu \in W^{k-1/2, p} (\partial \Omega)$ \cite{evans2022partial}. 
Typically, when finite element methods
are used for single-physics problems then there is no need to pay attention to 
the fractional norms when implementing the boundary conditions. Boundary conditions
are assigned at the boundary in a straightforward way as will be discussed in the next
chapter. However, for multi-physics problems, care may be required for efficient and
accurate simulation.   







\section{Exercises}

\begin{exercise}
What is a norm? Shown that 
\[
\|u\|_p = (\int_\Omega |u|^p \, dx)^{1/p} .    
\]
defines a norm on $L^p(\Omega)$. 

\end{exercise}

\begin{exercise}
What is an inner product? 
Show that 
\[
	(u, v)_{k} = \sum_{i \le k} \int_\Omega (\frac{\partial u}{\partial x})^i (\frac{\partial v}{\partial x})^i \,  dx.    
\]
defines an inner product on $H^1_0 (\Omega)$ for $\Omega=(0,1)$.   

\end{exercise}

\begin{exercise}
Compute the $H^1$ and $L^2$ norms of a random function with values
in $(0,1)$ on meshes representing the unit interval of width $10$, $100$, and $1000$ cells.   
\end{exercise}

\begin{exercise}
Compute the $H^1$ and $L^2$ norms of  $\sin( k \pi x)$ on the unit interval analytically 
and compare with the values presented in Table \ref{table:norms}.   
\end{exercise}

\begin{exercise}
\label{ex:piecewise}
Compute the $H^1$ and $L^2$ norms of  the hat function in Picture \ref{fig:piecewise}.   
\end{exercise}



\begin{exercise}
\label{ex:hat2}
Consider the following finite element function $u$ defined
as  
\[
u = \Bigg\{ 
\begin{array}{ll} \frac{1}{h} x - \frac{1}{h} (0.5 - h), & x=(0.5-h,0.5) \\  
              -\frac{1}{h} x + \frac{1}{h} (0.5 - h),  & x=(0.5,0.5 + h) \\  
                0, & \mbox{elsewhere}  
\end{array}
\]
That is, it corresponds to the hat function in Figure \ref{fig:piecewise}, where
$u(0.5)=1$ and the hat function is zero every where in $(0,0.5-h)$ and  $(0.5+h, 1)$.  
Compute the $H^1$ and $L^2$ norms of this function analytically, and the 
$L^2$, $H^1$ and $H^{-1}$ norms numerically for $h=10$, $100$ and $1000$.    
\end{exercise}






